{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model works well, but from what I notice sometimes it will misrecognize things due to commas or other non-character characters (though the miss rate is not high). To fix this (on initial impressions) we should preprocess the text by removing those characters using regex, and we can find the names based on the index of the returned array of labels compared to the list of words (split by new lines and whitespace) for the names. And just accomodate for when ' will get taken out and end up with Justin's as Justins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Dataset References:\n",
    "- CoNLL-2003: Tjong Kim Sang, Erik F., and De Meulder, Fien.\n",
    "  \"Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition.\" \n",
    "  Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003.\n",
    "  https://www.aclweb.org/anthology/W03-0419\n",
    "- OntoNotes 5.0: Weischedel, Ralph, et al. \"OntoNotes Release 5.0.\" LDC2013T19, Linguistic Data Consortium, 2013.\n",
    "  https://aclanthology.org/W13-3516\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from transformers import AlbertTokenizer, AlbertModel, AdamW\n",
    "from datasets import load_dataset\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import ast\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conll_dataset = load_dataset(\"conll2003\", trust_remote_code=True)\n",
    "ontonotes_dataset = load_dataset(\"ontonotes/conll2012_ontonotesv5\", \"english_v12\", trust_remote_code=True)\n",
    "tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONLL_2003_LABEL_MAP = {0: 0, 1: 1, 2: 1, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0}\n",
    "ONTONOTES_LABEL_MAP = {0: 0, 1: 1, 2: 1, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, \n",
    "                       11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, \n",
    "                       21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, \n",
    "                       31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0}\n",
    "WINDOW_SIZE = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NameDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (self.data[idx][\"input_ids\"].squeeze(0), \n",
    "                self.data[idx][\"attention_mask\"].squeeze(0), \n",
    "                self.data[idx][\"label\"])\n",
    "    \n",
    "def create_context_window(words, i):\n",
    "    start = max(0, i - WINDOW_SIZE)\n",
    "    end = min(len(words), i + WINDOW_SIZE + 1)\n",
    "\n",
    "    beginning_pad = [\"[PAD]\"] * (max(0, WINDOW_SIZE - i))\n",
    "    ending_pad = [\"[PAD]\"] * (max(0, (i + WINDOW_SIZE + 1) - len(words)))\n",
    "\n",
    "    context_window = [\"[CLS]\"] + beginning_pad + words[start:i] + [\"<w>\"] + [words[i]] + [\"</w>\"] + words[i+1:end] + ending_pad + [\"[SEP]\"]\n",
    "    return context_window\n",
    "\n",
    "def create_training_data(dataset_split):\n",
    "    data = []\n",
    "\n",
    "    for example in tqdm(dataset_split):\n",
    "        label_map = CONLL_2003_LABEL_MAP if example['dataset'] == 'conll' else ONTONOTES_LABEL_MAP\n",
    "        words = example[\"tokens\"]\n",
    "        labels = example[\"ner_tags\"]\n",
    "\n",
    "        for i, word in enumerate(words):\n",
    "            # Convert labels to binary (1 = name, 0 = not a name)\n",
    "            label = label_map[labels[i]]\n",
    "\n",
    "            context_window = create_context_window(words, i)\n",
    "\n",
    "            # Tokenize\n",
    "            encoding = tokenizer(context_window, padding=\"max_length\", max_length=35, truncation=True, is_split_into_words=True, return_tensors=\"pt\")\n",
    "\n",
    "            data.append({\n",
    "                \"input_ids\": encoding[\"input_ids\"],\n",
    "                \"attention_mask\": encoding[\"attention_mask\"],\n",
    "                \"label\": torch.tensor(label, dtype=torch.float)\n",
    "            })\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_conll(dataset_split, keep_non_name_ratio=0.5):\n",
    "    sentences_with_names = []\n",
    "    sentences_without_names = []\n",
    "\n",
    "    for example in dataset_split:\n",
    "        words = example[\"tokens\"]\n",
    "        labels = example[\"ner_tags\"]\n",
    "\n",
    "        if 1 in labels or 2 in labels:\n",
    "            sentences_with_names.append({\"dataset\": 'conll', \"tokens\": words, \"ner_tags\": labels})\n",
    "        else:\n",
    "            sentences_without_names.append({\"dataset\": 'conll', \"tokens\": words, \"ner_tags\": labels})\n",
    "    \n",
    "    num_non_name_sentences = int(len(sentences_with_names) * keep_non_name_ratio)\n",
    "    sentences_without_names = random.sample(sentences_without_names, min(num_non_name_sentences, len(sentences_without_names)))\n",
    "\n",
    "    filtered_dataset = sentences_with_names + sentences_without_names\n",
    "    random.shuffle(filtered_dataset)\n",
    "    return filtered_dataset\n",
    "\n",
    "def filter_ontonotes(dataset_split, keep_non_name_ratio=0.5):\n",
    "    sentences_with_names = []\n",
    "    sentences_without_names = []\n",
    "\n",
    "    for document in dataset_split:\n",
    "        for sentence in document[\"sentences\"]:\n",
    "            words = sentence[\"words\"]\n",
    "            labels = sentence[\"named_entities\"]\n",
    "\n",
    "            if 1 in labels or 2 in labels:\n",
    "                sentences_with_names.append({\"dataset\": 'ontonotes', \"tokens\": words, \"ner_tags\": labels})\n",
    "            else:\n",
    "                sentences_without_names.append({\"dataset\": 'ontonotes', \"tokens\": words, \"ner_tags\": labels})\n",
    "    \n",
    "    num_non_name_sentences = int(len(sentences_with_names) * keep_non_name_ratio)\n",
    "    sentences_without_names = random.sample(sentences_without_names, min(num_non_name_sentences, len(sentences_without_names)))\n",
    "\n",
    "    filtered_dataset = sentences_with_names + sentences_without_names\n",
    "    random.shuffle(filtered_dataset)\n",
    "    return filtered_dataset\n",
    "\n",
    "def filter_dataset(dataset_split, keep_non_name_ratio=0.5, label_map='conll'):\n",
    "    if label_map == 'conll':\n",
    "        return filter_conll(dataset_split, keep_non_name_ratio)\n",
    "    elif label_map == 'ontonotes':\n",
    "        return filter_ontonotes(dataset_split, keep_non_name_ratio)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid label_map\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating training data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [00:21<00:00, 141.62it/s]\n",
      "100%|██████████| 3000/3000 [00:30<00:00, 98.71it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating validation data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1344/1344 [00:10<00:00, 125.47it/s]\n",
      "100%|██████████| 2930/2930 [00:29<00:00, 98.48it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating test data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1230/1230 [00:08<00:00, 146.75it/s]\n",
      "100%|██████████| 2028/2028 [00:18<00:00, 108.66it/s]\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating training data\")\n",
    "filtered_conll_train = filter_dataset(conll_dataset[\"train\"], keep_non_name_ratio=0.2, label_map='conll')\n",
    "filtered_ontonotes_train = filter_dataset(ontonotes_dataset[\"train\"], keep_non_name_ratio=0.2, label_map='ontonotes')\n",
    "train_data = create_training_data(filtered_conll_train[:3000]) + create_training_data(filtered_ontonotes_train[:3000])\n",
    "print(\"Creating validation data\")\n",
    "filtered_conll_val = filter_dataset(conll_dataset[\"validation\"], keep_non_name_ratio=0.2, label_map='conll')\n",
    "filtered_ontonotes_val = filter_dataset(ontonotes_dataset[\"validation\"], keep_non_name_ratio=0.2, label_map='ontonotes')\n",
    "val_data = create_training_data(filtered_conll_val) + create_training_data(filtered_ontonotes_val)\n",
    "print(\"Creating test data\")\n",
    "filtered_conll_dataset = filter_dataset(conll_dataset[\"test\"], keep_non_name_ratio=0.2, label_map='conll')\n",
    "filtered_ontonotes_dataset = filter_dataset(ontonotes_dataset[\"test\"], keep_non_name_ratio=0.2, label_map='ontonotes')\n",
    "test_data = create_training_data(filtered_conll_dataset) + create_training_data(filtered_ontonotes_dataset)\n",
    "\n",
    "train_dataset = NameDataset(train_data)\n",
    "val_dataset = NameDataset(val_data)\n",
    "test_dataset = NameDataset(test_data)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NameClassifier(nn.Module):\n",
    "    def __init__(self, bert_model_name=\"albert-base-v2\"):\n",
    "        super(NameClassifier, self).__init__()\n",
    "        self.bert = AlbertModel.from_pretrained(bert_model_name)\n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids, attention_mask)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
    "        logits = self.fc(cls_output)\n",
    "        return self.sigmoid(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\justi\\.conda\\envs\\torch\\Lib\\site-packages\\transformers\\optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = NameClassifier().to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4083/4083 [04:18<00:00, 15.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 0.03678797635511407\n",
      "Validation Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3120/3120 [01:02<00:00, 49.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Validation Loss: 0.02933276888953771, Validation Accuracy: 0.9907429670593892\n",
      "New best model found\n",
      "Training Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4083/4083 [04:17<00:00, 15.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Training Loss: 0.01717848834009141\n",
      "Validation Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3120/3120 [01:02<00:00, 49.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Validation Loss: 0.02310355506934474, Validation Accuracy: 0.9923859902220085\n",
      "New best model found\n",
      "Training Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4083/4083 [04:19<00:00, 15.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Training Loss: 0.011761123314935557\n",
      "Validation Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3120/3120 [01:03<00:00, 49.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Validation Loss: 0.025575277589910118, Validation Accuracy: 0.9925362667307847\n",
      "Training Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4083/4083 [04:19<00:00, 15.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Training Loss: 0.008746734285134937\n",
      "Validation Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3120/3120 [01:03<00:00, 49.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Validation Loss: 0.029495563458411384, Validation Accuracy: 0.9925262482968662\n",
      "Training Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4083/4083 [04:19<00:00, 15.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Training Loss: 0.006615021663927405\n",
      "Validation Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3120/3120 [01:03<00:00, 49.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Validation Loss: 0.03436728044193259, Validation Accuracy: 0.9928568566161737\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "PATIENCE = 3\n",
    "\n",
    "best_val_loss = np.inf\n",
    "patience_counter = 0\n",
    "best_model_state = None\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Training Epoch {epoch+1}\")\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in tqdm(train_loader):\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}, Training Loss: {avg_loss}\")\n",
    "\n",
    "    print(f\"Validation Epoch {epoch+1}\")\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader):\n",
    "            input_ids, attention_mask, labels = batch\n",
    "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs.squeeze(), labels.float())\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = torch.round(outputs.squeeze())\n",
    "            total_correct += (preds == labels).sum().item()\n",
    "        \n",
    "        avg_loss = total_loss / len(val_loader)\n",
    "        accuracy = total_correct / len(val_dataset)\n",
    "        print(f\"Epoch {epoch+1}, Validation Loss: {avg_loss}, Validation Accuracy: {accuracy}\")\n",
    "    \n",
    "    if avg_loss < best_val_loss:\n",
    "        best_val_loss = avg_loss\n",
    "        patience_counter = 0\n",
    "        best_model_state = model.state_dict()\n",
    "        print(\"New best model found\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    if patience_counter >= PATIENCE:\n",
    "        print(\"Early stopping\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded best model\n"
     ]
    }
   ],
   "source": [
    "if best_model_state is not None:\n",
    "    model.load_state_dict(best_model_state)\n",
    "    print(\"Loaded best model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on testing set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2186/2186 [00:44<00:00, 48.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.044540655430173236, Accuracy: 27.110899809828847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing on testing set\")\n",
    "model.eval()\n",
    "total_loss = 0\n",
    "total_correct = 0\n",
    "for batch in tqdm(test_loader):\n",
    "    input_ids, attention_mask, labels = batch\n",
    "    input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "    outputs = model(input_ids, attention_mask)\n",
    "    loss = criterion(outputs.squeeze(), labels.float())\n",
    "    total_loss += loss.item()\n",
    "\n",
    "    predictions = (outputs > 0.5).float()\n",
    "    total_correct += (predictions == labels).sum().item()\n",
    "\n",
    "avg_loss = total_loss / len(test_loader)\n",
    "accuracy = total_correct / len(test_dataset)\n",
    "print(f\"Loss: {avg_loss}, Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 56.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matthew\t\t1.0\n",
      "and\t\t0.0\n",
      "Chloe\t\t1.0\n",
      "ran\t\t0.0\n",
      "into\t\t0.0\n",
      "William\t\t1.0\n",
      "at\t\t0.0\n",
      "the\t\t0.0\n",
      "museum\t\t0.0\n",
      "last\t\t0.0\n",
      "weekend\t\t0.0\n",
      "They\t\t0.0\n",
      "all\t\t0.0\n",
      "decided\t\t0.0\n",
      "to\t\t0.0\n",
      "explore\t\t0.0\n",
      "a\t\t0.0\n",
      "new\t\t0.0\n",
      "photography\t\t0.0\n",
      "exhibit\t\t0.0\n",
      "together\t\t0.0\n",
      "On\t\t0.0\n",
      "their\t\t0.0\n",
      "way\t\t0.0\n",
      "out\t\t0.0\n",
      "they\t\t0.0\n",
      "saw\t\t0.0\n",
      "Isabella\t\t1.0\n",
      "and\t\t0.0\n",
      "Daniel\t\t1.0\n",
      "who\t\t0.0\n",
      "invited\t\t0.0\n",
      "them\t\t0.0\n",
      "to\t\t0.0\n",
      "a\t\t0.0\n",
      "rooftop\t\t0.0\n",
      "dinner\t\t0.0\n",
      "later\t\t0.0\n",
      "that\t\t0.0\n",
      "evening\t\t0.0\n"
     ]
    }
   ],
   "source": [
    "test_string = \"\"\"\n",
    "Matthew and Chloe ran into William at the museum last weekend. They all decided to explore a new photography exhibit together. On their way out, they saw Isabella and Daniel, who invited them to a rooftop dinner later that evening.\n",
    "\"\"\"\n",
    "import re\n",
    "\n",
    "test_string = re.sub(r'[^\\w\\s]', '', test_string)\n",
    "test_tokens = test_string.split()\n",
    "\n",
    "test_data = create_training_data([{\"dataset\": 'conll', \"tokens\": test_tokens, \"ner_tags\": [0] * len(test_tokens)}])\n",
    "test_dataset = NameDataset(test_data)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "prediction = []\n",
    "for batch in test_loader:\n",
    "    input_ids, attention_mask, labels = batch\n",
    "    input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "    outputs = model(input_ids, attention_mask)\n",
    "    predictions = (outputs > 0.5).float()\n",
    "    prediction.extend(predictions.squeeze().tolist())\n",
    "\n",
    "for token, pred in zip(test_tokens, prediction):\n",
    "    print(f\"{token}\\t\\t{pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"name_classifier_2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "txt_dir = os.path.join(parent_dir, \"Regexs\", \"data\", \"ehr JMS.txt\")\n",
    "full_text = str()\n",
    "with open(txt_dir, \"r\") as f:\n",
    "    full_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text = full_text.replace(\"\\n\", \" \")\n",
    "full_text = re.sub(r'[^\\w\\s]', '', full_text)\n",
    "full_text = full_text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_name(model, text):\n",
    "    data = create_training_data([{\"dataset\": 'conll', \"tokens\": text, \"ner_tags\": [0] * len(text)}])\n",
    "    dataset = NameDataset(data)\n",
    "    loader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    model.eval()\n",
    "    prediction = []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            input_ids, attention_mask, labels = batch\n",
    "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            predictions = (outputs > 0.5).float()\n",
    "            prediction.extend(predictions.squeeze().tolist())\n",
    "    \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  5.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patient\t\t0.0\n",
      "Johnathan\t\t1.0\n",
      "Smith\t\t1.0\n",
      "Date\t\t0.0\n",
      "of\t\t0.0\n",
      "Birth\t\t0.0\n",
      "03151982\t\t0.0\n",
      "Medical\t\t0.0\n",
      "Record\t\t0.0\n",
      "Number\t\t0.0\n",
      "1234567\t\t0.0\n",
      "Date\t\t0.0\n",
      "of\t\t0.0\n",
      "Visit\t\t0.0\n",
      "10262023\t\t0.0\n",
      "Address\t\t0.0\n",
      "123\t\t0.0\n",
      "Oak\t\t0.0\n",
      "Street\t\t0.0\n",
      "Apt\t\t0.0\n",
      "13bBander\t\t0.0\n",
      "CA\t\t0.0\n",
      "91234\t\t0.0\n",
      "Phone\t\t0.0\n",
      "2317741234\t\t0.0\n",
      "email\t\t0.0\n",
      "jmichaelattcom\t\t0.0\n",
      "Provider\t\t0.0\n",
      "Dr\t\t0.0\n",
      "Eleanor\t\t1.0\n",
      "Vance\t\t1.0\n",
      "MD\t\t0.0\n",
      "Chief\t\t0.0\n",
      "Complaint\t\t0.0\n",
      "Persistent\t\t0.0\n",
      "cough\t\t0.0\n",
      "and\t\t0.0\n",
      "shortness\t\t0.0\n",
      "of\t\t0.0\n",
      "breath\t\t0.0\n",
      "for\t\t0.0\n",
      "the\t\t0.0\n",
      "past\t\t0.0\n",
      "week\t\t0.0\n",
      "History\t\t0.0\n",
      "of\t\t0.0\n",
      "Present\t\t0.0\n",
      "Illness\t\t0.0\n",
      "HPI\t\t0.0\n",
      "Mr\t\t0.0\n",
      "Smith\t\t1.0\n",
      "presents\t\t0.0\n",
      "to\t\t0.0\n",
      "the\t\t0.0\n",
      "clinic\t\t0.0\n",
      "with\t\t0.0\n",
      "a\t\t0.0\n",
      "oneweek\t\t0.0\n",
      "history\t\t0.0\n",
      "of\t\t0.0\n",
      "a\t\t0.0\n",
      "dry\t\t0.0\n",
      "hacking\t\t0.0\n",
      "cough\t\t0.0\n",
      "and\t\t0.0\n",
      "progressive\t\t0.0\n",
      "shortness\t\t0.0\n",
      "of\t\t0.0\n",
      "breath\t\t0.0\n",
      "He\t\t0.0\n",
      "reports\t\t0.0\n",
      "the\t\t0.0\n",
      "cough\t\t0.0\n",
      "began\t\t0.0\n",
      "insidiously\t\t0.0\n",
      "initially\t\t0.0\n",
      "mild\t\t0.0\n",
      "but\t\t0.0\n",
      "has\t\t0.0\n",
      "worsened\t\t0.0\n",
      "over\t\t0.0\n",
      "the\t\t0.0\n",
      "past\t\t0.0\n",
      "three\t\t0.0\n",
      "days\t\t0.0\n",
      "He\t\t0.0\n",
      "denies\t\t0.0\n",
      "fever\t\t0.0\n",
      "chills\t\t0.0\n",
      "or\t\t0.0\n",
      "night\t\t0.0\n",
      "sweats\t\t0.0\n",
      "He\t\t0.0\n",
      "describes\t\t0.0\n",
      "the\t\t0.0\n",
      "shortness\t\t0.0\n",
      "of\t\t0.0\n",
      "breath\t\t0.0\n",
      "as\t\t0.0\n",
      "a\t\t0.0\n",
      "feeling\t\t0.0\n",
      "of\t\t0.0\n",
      "tightness\t\t0.0\n",
      "in\t\t0.0\n",
      "his\t\t0.0\n",
      "chest\t\t0.0\n",
      "exacerbated\t\t0.0\n",
      "by\t\t0.0\n",
      "exertion\t\t0.0\n",
      "He\t\t0.0\n",
      "has\t\t0.0\n",
      "noticed\t\t0.0\n",
      "a\t\t0.0\n",
      "decrease\t\t0.0\n",
      "in\t\t0.0\n",
      "his\t\t0.0\n",
      "exercise\t\t0.0\n",
      "tolerance\t\t0.0\n",
      "He\t\t0.0\n",
      "denies\t\t0.0\n",
      "any\t\t0.0\n",
      "recent\t\t0.0\n",
      "travel\t\t0.0\n",
      "or\t\t0.0\n",
      "known\t\t0.0\n",
      "exposures\t\t0.0\n",
      "to\t\t0.0\n",
      "sick\t\t0.0\n",
      "individuals\t\t0.0\n",
      "He\t\t0.0\n",
      "has\t\t0.0\n",
      "tried\t\t0.0\n",
      "overthecounter\t\t0.0\n",
      "cough\t\t0.0\n",
      "suppressants\t\t0.0\n",
      "with\t\t0.0\n",
      "minimal\t\t0.0\n",
      "relief\t\t0.0\n",
      "Past\t\t0.0\n",
      "Medical\t\t0.0\n",
      "History\t\t0.0\n",
      "PMH\t\t0.0\n",
      "Asthma\t\t0.0\n",
      "diagnosed\t\t0.0\n",
      "in\t\t0.0\n",
      "childhood\t\t0.0\n",
      "Seasonal\t\t0.0\n",
      "allergies\t\t0.0\n",
      "No\t\t0.0\n",
      "known\t\t0.0\n",
      "surgical\t\t0.0\n",
      "history\t\t0.0\n",
      "Medications\t\t0.0\n",
      "Albuterol\t\t1.0\n",
      "inhaler\t\t0.0\n",
      "as\t\t0.0\n",
      "needed\t\t0.0\n",
      "Fluticasone\t\t0.0\n",
      "nasal\t\t0.0\n",
      "spray\t\t0.0\n",
      "daily\t\t0.0\n",
      "Allergies\t\t0.0\n",
      "Penicillin\t\t1.0\n",
      "rash\t\t1.0\n",
      "Family\t\t0.0\n",
      "History\t\t0.0\n",
      "Mother\t\t0.0\n",
      "Hypertension\t\t0.0\n",
      "type\t\t0.0\n",
      "2\t\t0.0\n",
      "diabetes\t\t0.0\n",
      "Father\t\t0.0\n",
      "Coronary\t\t0.0\n",
      "artery\t\t0.0\n",
      "disease\t\t0.0\n",
      "Social\t\t0.0\n",
      "History\t\t0.0\n",
      "Nonsmoker\t\t0.0\n",
      "Occasional\t\t0.0\n",
      "alcohol\t\t0.0\n",
      "consumption\t\t0.0\n",
      "12\t\t0.0\n",
      "drinks\t\t0.0\n",
      "per\t\t0.0\n",
      "week\t\t0.0\n",
      "Works\t\t0.0\n",
      "as\t\t0.0\n",
      "a\t\t0.0\n",
      "software\t\t0.0\n",
      "engineer\t\t0.0\n",
      "Reports\t\t0.0\n",
      "moderate\t\t0.0\n",
      "stress\t\t0.0\n",
      "levels\t\t0.0\n",
      "due\t\t0.0\n",
      "to\t\t0.0\n",
      "work\t\t0.0\n",
      "deadlines\t\t0.0\n",
      "Review\t\t0.0\n",
      "of\t\t0.0\n",
      "Systems\t\t0.0\n",
      "ROS\t\t0.0\n",
      "Constitutional\t\t0.0\n",
      "Denies\t\t0.0\n",
      "fever\t\t0.0\n",
      "chills\t\t0.0\n",
      "fatigue\t\t0.0\n",
      "or\t\t0.0\n",
      "weight\t\t0.0\n",
      "loss\t\t0.0\n",
      "Respiratory\t\t0.0\n",
      "Positive\t\t0.0\n",
      "for\t\t0.0\n",
      "cough\t\t0.0\n",
      "and\t\t0.0\n",
      "shortness\t\t0.0\n",
      "of\t\t0.0\n",
      "breath\t\t0.0\n",
      "Denies\t\t0.0\n",
      "hemoptysis\t\t1.0\n",
      "or\t\t0.0\n",
      "wheezing\t\t0.0\n",
      "Cardiovascular\t\t0.0\n",
      "Denies\t\t0.0\n",
      "chest\t\t0.0\n",
      "pain\t\t0.0\n",
      "palpitations\t\t0.0\n",
      "or\t\t0.0\n",
      "edema\t\t0.0\n",
      "Gastrointestinal\t\t0.0\n",
      "Denies\t\t0.0\n",
      "nausea\t\t0.0\n",
      "vomiting\t\t0.0\n",
      "or\t\t0.0\n",
      "abdominal\t\t0.0\n",
      "pain\t\t0.0\n",
      "Neurological\t\t0.0\n",
      "Denies\t\t0.0\n",
      "headache\t\t0.0\n",
      "dizziness\t\t0.0\n",
      "or\t\t0.0\n",
      "weakness\t\t0.0\n",
      "Physical\t\t0.0\n",
      "Examination\t\t0.0\n",
      "General\t\t0.0\n",
      "Alert\t\t0.0\n",
      "and\t\t0.0\n",
      "oriented\t\t0.0\n",
      "appears\t\t0.0\n",
      "uncomfortable\t\t0.0\n",
      "due\t\t0.0\n",
      "to\t\t0.0\n",
      "respiratory\t\t0.0\n",
      "distress\t\t0.0\n",
      "Vital\t\t0.0\n",
      "Signs\t\t0.0\n",
      "Blood\t\t0.0\n",
      "pressure\t\t0.0\n",
      "13080\t\t0.0\n",
      "mmHg\t\t0.0\n",
      "Heart\t\t0.0\n",
      "rate\t\t0.0\n",
      "90\t\t0.0\n",
      "bpm\t\t0.0\n",
      "Respiratory\t\t0.0\n",
      "rate\t\t0.0\n",
      "22\t\t0.0\n",
      "breathsmin\t\t0.0\n",
      "Oxygen\t\t0.0\n",
      "saturation\t\t0.0\n",
      "92\t\t0.0\n",
      "on\t\t0.0\n",
      "room\t\t0.0\n",
      "air\t\t0.0\n",
      "Temperature\t\t0.0\n",
      "986F\t\t0.0\n",
      "37C\t\t0.0\n",
      "Respiratory\t\t0.0\n",
      "Diminished\t\t0.0\n",
      "breath\t\t0.0\n",
      "sounds\t\t0.0\n",
      "bilaterally\t\t0.0\n",
      "with\t\t0.0\n",
      "scattered\t\t0.0\n",
      "wheezing\t\t0.0\n",
      "Cardiovascular\t\t0.0\n",
      "Regular\t\t0.0\n",
      "heart\t\t0.0\n",
      "rate\t\t0.0\n",
      "and\t\t0.0\n",
      "rhythm\t\t0.0\n",
      "no\t\t0.0\n",
      "murmurs\t\t0.0\n",
      "Abdomen\t\t0.0\n",
      "Soft\t\t0.0\n",
      "nontender\t\t0.0\n",
      "Extremities\t\t0.0\n",
      "No\t\t0.0\n",
      "edema\t\t0.0\n",
      "Assessment\t\t0.0\n",
      "Acute\t\t0.0\n",
      "exacerbation\t\t0.0\n",
      "of\t\t0.0\n",
      "asthma\t\t0.0\n",
      "Possible\t\t0.0\n",
      "bronchitis\t\t0.0\n",
      "Plan\t\t0.0\n",
      "Administered\t\t0.0\n",
      "nebulized\t\t0.0\n",
      "albuterol\t\t0.0\n",
      "in\t\t0.0\n",
      "clinic\t\t0.0\n",
      "Prescribed\t\t0.0\n",
      "prednisone\t\t0.0\n",
      "40mg\t\t0.0\n",
      "daily\t\t0.0\n",
      "for\t\t0.0\n",
      "5\t\t0.0\n",
      "days\t\t0.0\n",
      "Prescribed\t\t0.0\n",
      "inhaled\t\t0.0\n",
      "corticosteroid\t\t0.0\n",
      "fluticasonesalmeterol\t\t0.0\n",
      "twice\t\t0.0\n",
      "daily\t\t0.0\n",
      "Ordered\t\t0.0\n",
      "chest\t\t0.0\n",
      "Xray\t\t0.0\n",
      "to\t\t0.0\n",
      "rule\t\t0.0\n",
      "out\t\t0.0\n",
      "pneumonia\t\t0.0\n",
      "Instructed\t\t0.0\n",
      "patient\t\t0.0\n",
      "to\t\t0.0\n",
      "monitor\t\t0.0\n",
      "symptoms\t\t0.0\n",
      "and\t\t0.0\n",
      "return\t\t0.0\n",
      "to\t\t0.0\n",
      "clinic\t\t0.0\n",
      "if\t\t0.0\n",
      "symptoms\t\t0.0\n",
      "worsen\t\t0.0\n",
      "Patient\t\t0.0\n",
      "education\t\t0.0\n",
      "provided\t\t0.0\n",
      "on\t\t0.0\n",
      "proper\t\t0.0\n",
      "inhaler\t\t0.0\n",
      "technique\t\t0.0\n",
      "and\t\t0.0\n",
      "asthma\t\t0.0\n",
      "action\t\t0.0\n",
      "plan\t\t0.0\n",
      "Chest\t\t0.0\n",
      "XRay\t\t0.0\n",
      "Report\t\t0.0\n",
      "10262023\t\t0.0\n",
      "Mild\t\t0.0\n",
      "hyperinflation\t\t0.0\n",
      "of\t\t0.0\n",
      "the\t\t0.0\n",
      "lungs\t\t0.0\n",
      "No\t\t0.0\n",
      "evidence\t\t0.0\n",
      "of\t\t0.0\n",
      "pneumonia\t\t0.0\n",
      "or\t\t0.0\n",
      "other\t\t0.0\n",
      "acute\t\t0.0\n",
      "pulmonary\t\t0.0\n",
      "process\t\t0.0\n",
      "Followup\t\t0.0\n",
      "Appointment\t\t0.0\n",
      "Scheduled\t\t0.0\n",
      "for\t\t0.0\n",
      "11022023\t\t0.0\n",
      "Provider\t\t0.0\n",
      "Notes\t\t0.0\n",
      "Patient\t\t0.0\n",
      "responded\t\t0.0\n",
      "well\t\t0.0\n",
      "to\t\t0.0\n",
      "nebulized\t\t0.0\n",
      "albuterol\t\t0.0\n",
      "in\t\t0.0\n",
      "clinic\t\t0.0\n",
      "Patient\t\t0.0\n",
      "was\t\t0.0\n",
      "given\t\t0.0\n",
      "clear\t\t0.0\n",
      "instructions\t\t0.0\n",
      "on\t\t0.0\n",
      "medication\t\t0.0\n",
      "usage\t\t0.0\n",
      "and\t\t0.0\n",
      "when\t\t0.0\n",
      "to\t\t0.0\n",
      "return\t\t0.0\n",
      "to\t\t0.0\n",
      "clinic\t\t0.0\n",
      "Patient\t\t0.0\n",
      "understands\t\t0.0\n",
      "the\t\t0.0\n",
      "return\t\t0.0\n",
      "instructions\t\t0.0\n",
      "11022023\t\t0.0\n",
      "Followup\t\t0.0\n",
      "Patient\t\t0.0\n",
      "reports\t\t0.0\n",
      "significant\t\t0.0\n",
      "improvement\t\t0.0\n",
      "in\t\t0.0\n",
      "cough\t\t0.0\n",
      "and\t\t0.0\n",
      "shortness\t\t0.0\n",
      "of\t\t0.0\n",
      "breath\t\t0.0\n",
      "Reports\t\t0.0\n",
      "using\t\t0.0\n",
      "inhaler\t\t1.0\n",
      "as\t\t0.0\n",
      "instructed\t\t0.0\n",
      "Examination\t\t0.0\n",
      "shows\t\t0.0\n",
      "improved\t\t0.0\n",
      "breath\t\t0.0\n",
      "sounds\t\t0.0\n",
      "minimal\t\t0.0\n",
      "wheezing\t\t0.0\n",
      "Continue\t\t0.0\n",
      "current\t\t0.0\n",
      "medication\t\t0.0\n",
      "regimen\t\t0.0\n",
      "Reevaluate\t\t0.0\n",
      "in\t\t0.0\n",
      "1\t\t0.0\n",
      "month\t\t0.0\n"
     ]
    }
   ],
   "source": [
    "predictions = predict_name(model, full_text)\n",
    "for token, pred in zip(full_text, predictions):\n",
    "    print(f\"{token}\\t\\t{pred}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
