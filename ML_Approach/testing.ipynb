{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model works well, but from what I notice sometimes it will misrecognize things due to commas or other non-character characters (though the miss rate is not high). To fix this (on initial impressions) we should preprocess the text by removing those characters using regex, and we can find the names based on the index of the returned array of labels compared to the list of words (split by new lines and whitespace) for the names. And just accomodate for when ' will get taken out and end up with Justin's as Justins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Dataset References:\n",
    "- CoNLL-2003: Tjong Kim Sang, Erik F., and De Meulder, Fien.\n",
    "  \"Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition.\" \n",
    "  Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003.\n",
    "  https://www.aclweb.org/anthology/W03-0419\n",
    "- OntoNotes 5.0: Weischedel, Ralph, et al. \"OntoNotes Release 5.0.\" LDC2013T19, Linguistic Data Consortium, 2013.\n",
    "  https://aclanthology.org/W13-3516\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from transformers import AlbertTokenizer, AlbertModel, AdamW\n",
    "from datasets import load_dataset\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import ast\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conll_dataset = load_dataset(\"conll2003\", trust_remote_code=True)\n",
    "ontonotes_dataset = load_dataset(\"ontonotes/conll2012_ontonotesv5\", \"english_v12\", trust_remote_code=True)\n",
    "tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONLL_2003_LABEL_MAP = {0: 0, 1: 1, 2: 1, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0}\n",
    "ONTONOTES_LABEL_MAP = {0: 0, 1: 1, 2: 1, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, \n",
    "                       11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, \n",
    "                       21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, \n",
    "                       31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0}\n",
    "WINDOW_SIZE = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NameDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (self.data[idx][\"input_ids\"].squeeze(0), \n",
    "                self.data[idx][\"attention_mask\"].squeeze(0), \n",
    "                self.data[idx][\"label\"])\n",
    "    \n",
    "def create_context_window(words, i):\n",
    "    start = max(0, i - WINDOW_SIZE)\n",
    "    end = min(len(words), i + WINDOW_SIZE + 1)\n",
    "\n",
    "    beginning_pad = [\"[PAD]\"] * (max(0, WINDOW_SIZE - i))\n",
    "    ending_pad = [\"[PAD]\"] * (max(0, (i + WINDOW_SIZE + 1) - len(words)))\n",
    "\n",
    "    context_window = [\"[CLS]\"] + beginning_pad + words[start:i] + [\"<w>\"] + [words[i]] + [\"</w>\"] + words[i+1:end] + ending_pad + [\"[SEP]\"]\n",
    "    return context_window\n",
    "\n",
    "def create_training_data(dataset_split):\n",
    "    data = []\n",
    "\n",
    "    for example in tqdm(dataset_split):\n",
    "        label_map = CONLL_2003_LABEL_MAP if example['dataset'] == 'conll' else ONTONOTES_LABEL_MAP\n",
    "        words = example[\"tokens\"]\n",
    "        labels = example[\"ner_tags\"]\n",
    "\n",
    "        for i, word in enumerate(words):\n",
    "            # Convert labels to binary (1 = name, 0 = not a name)\n",
    "            label = label_map[labels[i]]\n",
    "\n",
    "            context_window = create_context_window(words, i)\n",
    "\n",
    "            # Tokenize\n",
    "            encoding = tokenizer(context_window, padding=\"max_length\", max_length=35, truncation=True, is_split_into_words=True, return_tensors=\"pt\")\n",
    "\n",
    "            data.append({\n",
    "                \"input_ids\": encoding[\"input_ids\"],\n",
    "                \"attention_mask\": encoding[\"attention_mask\"],\n",
    "                \"label\": torch.tensor(label, dtype=torch.float)\n",
    "            })\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_conll(dataset_split, keep_non_name_ratio=0.5):\n",
    "    sentences_with_names = []\n",
    "    sentences_without_names = []\n",
    "\n",
    "    for example in dataset_split:\n",
    "        words = example[\"tokens\"]\n",
    "        labels = example[\"ner_tags\"]\n",
    "\n",
    "        if 1 in labels or 2 in labels:\n",
    "            sentences_with_names.append({\"dataset\": 'conll', \"tokens\": words, \"ner_tags\": labels})\n",
    "        else:\n",
    "            sentences_without_names.append({\"dataset\": 'conll', \"tokens\": words, \"ner_tags\": labels})\n",
    "    \n",
    "    num_non_name_sentences = int(len(sentences_with_names) * keep_non_name_ratio)\n",
    "    sentences_without_names = random.sample(sentences_without_names, min(num_non_name_sentences, len(sentences_without_names)))\n",
    "\n",
    "    filtered_dataset = sentences_with_names + sentences_without_names\n",
    "    random.shuffle(filtered_dataset)\n",
    "    return filtered_dataset\n",
    "\n",
    "def filter_ontonotes(dataset_split, keep_non_name_ratio=0.5):\n",
    "    sentences_with_names = []\n",
    "    sentences_without_names = []\n",
    "\n",
    "    for document in dataset_split:\n",
    "        for sentence in document[\"sentences\"]:\n",
    "            words = sentence[\"words\"]\n",
    "            labels = sentence[\"named_entities\"]\n",
    "\n",
    "            if 1 in labels or 2 in labels:\n",
    "                sentences_with_names.append({\"dataset\": 'ontonotes', \"tokens\": words, \"ner_tags\": labels})\n",
    "            else:\n",
    "                sentences_without_names.append({\"dataset\": 'ontonotes', \"tokens\": words, \"ner_tags\": labels})\n",
    "    \n",
    "    num_non_name_sentences = int(len(sentences_with_names) * keep_non_name_ratio)\n",
    "    sentences_without_names = random.sample(sentences_without_names, min(num_non_name_sentences, len(sentences_without_names)))\n",
    "\n",
    "    filtered_dataset = sentences_with_names + sentences_without_names\n",
    "    random.shuffle(filtered_dataset)\n",
    "    return filtered_dataset\n",
    "\n",
    "def filter_dataset(dataset_split, keep_non_name_ratio=0.5, label_map='conll'):\n",
    "    if label_map == 'conll':\n",
    "        return filter_conll(dataset_split, keep_non_name_ratio)\n",
    "    elif label_map == 'ontonotes':\n",
    "        return filter_ontonotes(dataset_split, keep_non_name_ratio)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid label_map\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating training data\")\n",
    "filtered_conll_train = filter_dataset(conll_dataset[\"train\"], keep_non_name_ratio=0.2, label_map='conll')\n",
    "filtered_ontonotes_train = filter_dataset(ontonotes_dataset[\"train\"], keep_non_name_ratio=0.2, label_map='ontonotes')\n",
    "train_data = create_training_data(filtered_conll_train[:3000]) + create_training_data(filtered_ontonotes_train[:3000])\n",
    "print(\"Creating validation data\")\n",
    "filtered_conll_val = filter_dataset(conll_dataset[\"validation\"], keep_non_name_ratio=0.2, label_map='conll')\n",
    "filtered_ontonotes_val = filter_dataset(ontonotes_dataset[\"validation\"], keep_non_name_ratio=0.2, label_map='ontonotes')\n",
    "val_data = create_training_data(filtered_conll_val) + create_training_data(filtered_ontonotes_val)\n",
    "print(\"Creating test data\")\n",
    "filtered_conll_dataset = filter_dataset(conll_dataset[\"test\"], keep_non_name_ratio=0.2, label_map='conll')\n",
    "filtered_ontonotes_dataset = filter_dataset(ontonotes_dataset[\"test\"], keep_non_name_ratio=0.2, label_map='ontonotes')\n",
    "test_data = create_training_data(filtered_conll_dataset) + create_training_data(filtered_ontonotes_dataset)\n",
    "\n",
    "train_dataset = NameDataset(train_data)\n",
    "val_dataset = NameDataset(val_data)\n",
    "test_dataset = NameDataset(test_data)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NameClassifier(nn.Module):\n",
    "    def __init__(self, bert_model_name=\"albert-base-v2\"):\n",
    "        super(NameClassifier, self).__init__()\n",
    "        self.bert = AlbertModel.from_pretrained(bert_model_name)\n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids, attention_mask)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
    "        logits = self.fc(cls_output)\n",
    "        return self.sigmoid(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = NameClassifier().to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "PATIENCE = 3\n",
    "\n",
    "best_val_loss = np.inf\n",
    "patience_counter = 0\n",
    "best_model_state = None\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Training Epoch {epoch+1}\")\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in tqdm(train_loader):\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}, Training Loss: {avg_loss}\")\n",
    "\n",
    "    print(f\"Validation Epoch {epoch+1}\")\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader):\n",
    "            input_ids, attention_mask, labels = batch\n",
    "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs.squeeze(), labels.float())\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = torch.round(outputs.squeeze())\n",
    "            total_correct += (preds == labels).sum().item()\n",
    "        \n",
    "        avg_loss = total_loss / len(val_loader)\n",
    "        accuracy = total_correct / len(val_dataset)\n",
    "        print(f\"Epoch {epoch+1}, Validation Loss: {avg_loss}, Validation Accuracy: {accuracy}\")\n",
    "    \n",
    "    if avg_loss < best_val_loss:\n",
    "        best_val_loss = avg_loss\n",
    "        patience_counter = 0\n",
    "        best_model_state = model.state_dict()\n",
    "        print(\"New best model found\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    if patience_counter >= PATIENCE:\n",
    "        print(\"Early stopping\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if best_model_state is not None:\n",
    "    model.load_state_dict(best_model_state)\n",
    "    print(\"Loaded best model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing on testing set\")\n",
    "model.eval()\n",
    "total_loss = 0\n",
    "total_correct = 0\n",
    "for batch in tqdm(test_loader):\n",
    "    input_ids, attention_mask, labels = batch\n",
    "    input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "    outputs = model(input_ids, attention_mask)\n",
    "    loss = criterion(outputs.squeeze(), labels.float())\n",
    "    total_loss += loss.item()\n",
    "\n",
    "    predictions = (outputs > 0.5).float()\n",
    "    total_correct += (predictions == labels).sum().item()\n",
    "\n",
    "avg_loss = total_loss / len(test_loader)\n",
    "accuracy = total_correct / len(test_dataset)\n",
    "print(f\"Loss: {avg_loss}, Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_string = \"\"\"\n",
    "Matthew and Chloe ran into William at the museum last weekend. They all decided to explore a new photography exhibit together. On their way out, they saw Isabella and Daniel, who invited them to a rooftop dinner later that evening.\n",
    "\"\"\"\n",
    "import re\n",
    "\n",
    "test_string = re.sub(r'[^\\w\\s]', '', test_string)\n",
    "test_tokens = test_string.split()\n",
    "\n",
    "test_data = create_training_data([{\"dataset\": 'conll', \"tokens\": test_tokens, \"ner_tags\": [0] * len(test_tokens)}])\n",
    "test_dataset = NameDataset(test_data)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "prediction = []\n",
    "for batch in test_loader:\n",
    "    input_ids, attention_mask, labels = batch\n",
    "    input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "    outputs = model(input_ids, attention_mask)\n",
    "    predictions = (outputs > 0.5).float()\n",
    "    prediction.extend(predictions.squeeze().tolist())\n",
    "\n",
    "for token, pred in zip(test_tokens, prediction):\n",
    "    print(f\"{token}\\t\\t{pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"name_classifier_2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "txt_dir = os.path.join(parent_dir, \"Regexs\", \"data\", \"ehr JMS.txt\")\n",
    "full_text = str()\n",
    "with open(txt_dir, \"r\") as f:\n",
    "    full_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text = full_text.replace(\"\\n\", \" \")\n",
    "full_text = re.sub(r'[^\\w\\s]', '', full_text)\n",
    "full_text = full_text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_name(model, text):\n",
    "    data = create_training_data([{\"dataset\": 'conll', \"tokens\": text, \"ner_tags\": [0] * len(text)}])\n",
    "    dataset = NameDataset(data)\n",
    "    loader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    model.eval()\n",
    "    prediction = []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            input_ids, attention_mask, labels = batch\n",
    "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            predictions = (outputs > 0.5).float()\n",
    "            prediction.extend(predictions.squeeze().tolist())\n",
    "    \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict_name(model, full_text)\n",
    "for token, pred in zip(full_text, predictions):\n",
    "    print(f\"{token}\\t\\t{pred}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
